
#  pinheiro-roberta-goemotions 
#### üìä Estat√≠sticas: +40 downloads no [Hugging Face](https://huggingface.co/teu-usuario/teu-modelo) üòéüòéüòé

<!-- Resumo r√°pido do modelo -->

Este √© um modelo baseado em Roberta, fine-tuned para **classifica√ß√£o multi-label de emo√ß√µes** em textos usando o dataset **GoEmotions**. Ele classifica 28 categorias de emo√ß√£o simultaneamente.

### Descri√ß√£o do Modelo

Modelo **RobertaForSequenceClassification** fine-tuned para classifica√ß√£o multi-label de 28 emo√ß√µes:

`['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']`

- **Desenvolvido por:** Matheus Pinheiro
- **Tipo de modelo:** RobertaForSequenceClassification (multi-label)  
- **Idioma(s):** Ingl√™s
- **Licen√ßa:** MIT
- **Base do modelo:** `roberta-base`

---

### üõ†Ô∏è Tecnologias Utilizadas

Este projeto foi desenvolvido utilizando o seguinte stack de tecnologias de *Deep Learning*:

- **Framework de Deep Learning:** [PyTorch](https://pytorch.org/)  ‚Äî usado para o fine-tuning do modelo pr√©-treinado
- **Biblioteca de Modelagem:** [Hugging Face Transformers](https://huggingface.co/docs/transformers) ‚Äî usada para carregar o modelo base, o tokenizador e a *pipeline* de infer√™ncia  
- **Modelo Base:** [`roberta-base`](https://huggingface.co/roberta-base)  
- **Linguagem:** Python  
- **Ecossistema:** [Hugging Face Hub](https://huggingface.co/) ‚Äî utilizado para hospedagem do modelo e do tokenizador  

### Uso Direto

Classifica√ß√£o multi-label de emo√ß√µes em textos em ingl√™s. Retorna a probabilidade de cada emo√ß√£o para o texto dado.

## Vi√©s, Riscos e Limita√ß√µes

- Treinado no dataset GoEmotions (ingl√™s) e pode n√£o generalizar para outros idiomas ou contextos culturais.  
- Algumas emo√ß√µes podem ter probabilidades baixas mesmo quando presentes.  
- As probabilidades multi-label s√£o relativas, n√£o valores absolutos de emo√ß√£o.  

### Recomenda√ß√µes

- Interpretar as probabilidades como **scores relativos**, n√£o como r√≥tulos absolutos.

## Como Usar o Modelo

```python
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification

# Carregar tokenizer
tokenizer =  AutoTokenizer.from_pretrained("roberta-base")

# Map de id2label
id2label = {
    0: 'admiration', 1: 'amusement', 2: 'anger', 3: 'annoyance', 4: 'approval',
    5: 'caring', 6: 'confusion', 7: 'curiosity', 8: 'desire', 9: 'disappointment',
    10: 'disapproval', 11: 'disgust', 12: 'embarrassment', 13: 'excitement', 14: 'fear',
    15: 'gratitude', 16: 'grief', 17: 'joy', 18: 'love', 19: 'nervousness', 20: 'optimism',
    21: 'pride', 22: 'realization', 23: 'relief', 24: 'remorse', 25: 'sadness', 26: 'surprise', 27: 'neutral'
}

# Carregar modelo
modelo = AutoModelForSequenceClassification.from_pretrained("pinheiroxs/pinheiro-roberta-goemotions",id2label=id2label)

# Criar pipeline
classifier = pipeline("text-classification", model=modelo, tokenizer=tokenizer, return_all_scores=True)
```
```python
# Exemplo de uso
texto = "I am very happy today!"
resultado = classifier(texto)[0]

# Formatar resultados
resultado_ordenado = sorted(resultado, key=lambda x: x['score'], reverse=True)
for r in resultado_ordenado:
    print(f"{r['label']:15} : {r['score']*100:.2f}%")
```

### Dados de Treinamento

- **Dataset:** GoEmotions
- **N√∫mero de r√≥tulos:** 28 emo√ß√µes em textos em ingl√™s

#### Pr√©-processamento

- Tokeniza√ß√£o padr√£o usando `AutoTokenizer`
- Binariza√ß√£o multi-label dos r√≥tulos de emo√ß√£o

#### Hiperpar√¢metros de Treinamento

- **Regime de treino:** 4 epochs
- **Batch size:** 16
- **Otimizador:** AdamW
- **Learning rate:** 2e-5
- **Hidden size:** 768
- **Intermediate size:** 3072
- **N√∫mero de attention heads:** 12
- **N√∫mero de camadas ocultas:** 12
- **Dropout:** 0.1
